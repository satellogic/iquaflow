{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2586033b-4a2f-411a-b5e6-05159822626f",
   "metadata": {},
   "source": [
    "# User Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17affdf-979d-4059-8dc9-62dee242d0eb",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "This is a summary of a complete typical workflow.\n",
    "\n",
    " 1. Define the original dataset with DatasetWrapper.\n",
    "\n",
    "```python\n",
    "from iquaflow.datasets import DSWrapper\n",
    "\n",
    "ds_wrapper = DSWrapper(data_path=data_path)\n",
    "```\n",
    "\n",
    " 2. Define the modifications intended for each experiment. In this case JPG\n",
    "Modifiers with quality from 10 to 90.\n",
    "\n",
    "```python\n",
    "from iquaflow.datasets import DSModifier_jpg\n",
    "\n",
    "ds_modifiers_list = [DSModifier_jpg(params={'quality': i}) for i in [10,30,50,70,90] ]\n",
    "```\n",
    "\n",
    " 3. Define the model execution method. In this case the training method is a python script, so we define PythonScriptExecutionTask. Additionally hyperparameter variations can be added. In this case the epochs and learning rate is varied. The tool will then loop through all possible combinations of the variations. The user can also set the number of repetitions.\n",
    "\n",
    "\n",
    "```python\n",
    "from iquaflow.experiments import ExperimentSetup\n",
    "\n",
    "experiment = ExperimentSetup(\n",
    "   experiment_name=\"MyFirstExperiment\",\n",
    "   task_instance=task,\n",
    "   ref_dsw_train=ds_wrapper,\n",
    "   ds_modifiers_list=ds_modifiers_list,\n",
    "   repetitions=5,\n",
    "   extra_train_params={\n",
    "      'epochs':[10,15,20],\n",
    "      'lr':[1e-5,1e-6,1e-7]\n",
    "   }\n",
    ")\n",
    "\n",
    "experiment.execute()\n",
    "\n",
    "```\n",
    "\n",
    " 4. The information from the executed experiment can be collected in a json. Also a dataframe suitable for visualization tools (see next step) can be extracted from\n",
    "\n",
    "```python\n",
    "from iquaflow.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "\n",
    "runs = experiment_info.runs\n",
    "\n",
    "df = experiment_info.get_df(\n",
    "   ds_params=[\"modifier\"],\n",
    "   metrics=['rmse','epochs','lr'],\n",
    "   dropna = True,\n",
    "   fields_to_float_lst = ['rmse','lr'],\n",
    "   fields_to_int_lst = ['epochs']\n",
    ")\n",
    "```\n",
    "\n",
    " 5. Visualizations can be made from the tool. In this case plots of root mean square error against learning rate variations are showed. This is one plot for each epoch (legend) in a shared chart.\n",
    "\n",
    "\n",
    "```python\n",
    "from iquaflow.experiments import ExperimentVisual\n",
    "\n",
    "ev = ExperimentVisual(\n",
    "   df,\n",
    "   os.path.join(data_path, \"mod-rmse-lr-epoch.png\")\n",
    ")\n",
    "\n",
    "ev.visualize(\n",
    "   xvar=\"lr\",\n",
    "   yvar=\"rmse\",\n",
    "   legend_var=\"epochs\",\n",
    "   title=\"rmse - lr\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28767d98-48e6-4eaa-8de9-05196bca78ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conventions\n",
    "\n",
    "In **iquaflow** conventions are prefered over configurations. \n",
    "\n",
    "### Dataset Formats\n",
    "\n",
    "*  iquaflow understands a dataset as a folder containing a sub-folder\n",
    "   with images and ground truth in json format. Datasets that does not\n",
    "   follow this format should be changed in order to perform experiments.\n",
    "\n",
    "*  In case of detection or segmentation tasks, the preferred formats\n",
    "   are:\n",
    "\n",
    "   *  Json in COCO format.\n",
    "   *  GeoJson with the minimum required fields (\"image_filename\", \"class_id\", \"geometry\").\n",
    "   *  A folder named maskes with images corresponding to the segmentation annotations.\n",
    "\n",
    "*  iquaflow primarily works with COCO json ground truth adopted by most\n",
    "   of the datasets and models of the field. In case that the dataset is\n",
    "   in other format, the user can transform it to COCO\n",
    "   https://blog.roboflow.ai/how-to-convert-annotations-from-voc-xml-to-coco-json/\n",
    "   Otherwise, iquaflow can not perform sanity neither statistics checks\n",
    "\n",
    "*  For other kind of tasks, such as image generation, it is only\n",
    "   necessary to have the ground truth in a json format. Alternatively,\n",
    "   iquaflow can recognize a dataset without any ground truth file\n",
    "\n",
    "*  When the dataset is modified, iquaflow creates a modified copy of the\n",
    "   dataset in its parent folder. As a convention, iquaflow adds to the name\n",
    "   of the original dataset a “#” followed by the name of the\n",
    "   modification as you can see in the following image.\n",
    "   \n",
    "### Training script\n",
    "\n",
    "The training script requires these arguments:\n",
    "\n",
    "* outputpath\n",
    "* trainds\n",
    "* valds (opt)\n",
    "* testds (opt)\n",
    "* mlfuri (opt)\n",
    "* mlfexpid (opt)\n",
    "* mlfrunid (opt)\n",
    "* other hyperparameters (opt)\n",
    "\n",
    "The arguments marked with (opt) are optional. Arguments starting with **mlf** are used when the flag **mlflow_monitoring** is activated in the **ExperimentSetup**\n",
    " \n",
    "### Output Formats\n",
    "\n",
    "The packaged model could write in the output temporary folder the following files in order to be parsed as experiment parameters and metrics:\n",
    "\n",
    "* **results.json**: Json with keys as the name of parameter, values as a number related to the metric or an array reference to a sequence of values of that parameter.\n",
    "\n",
    "```python\n",
    "{\n",
    " \"train_f1\": 0.83,\n",
    " \"val_f1\": 0.78,\n",
    " \"test_f1\": 0.79,\n",
    " \"train_focal_loss\": [1.34, 1.29, 1.24, …., 0.01]\n",
    " \"val_focal_loss\": [1.34, 1.29, 1.24, …., 0.01]\n",
    "}\n",
    "```\n",
    "\n",
    "* **output.json** : Output of the model (this allows to avoid reproducing experiments in the future in case it is wanted to test a new metric for former experiments) in a folder named output. The format of this json file depends on the task of the DL model.\n",
    "\n",
    "* Bounding Box Detection: **output.json** consists of a COCO format json, containing as many elements as detections have been made in the dataset. Each of these elements looks as shown below.\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"image_id\" : 85\n",
    "    \"iscrowd\" : 0\n",
    "    \"bbox\":[\n",
    "        522.5372924804688\n",
    "        474.1499938964844\n",
    "        28.968505859375\n",
    "        27.19696044921875\n",
    "    ]\n",
    "    \"area\": 2427.050960971974\n",
    "    \"category_id\": 1\n",
    "    \"id\": 1\n",
    "    score : 0.9709288477897644\n",
    "}\n",
    "```\n",
    "\n",
    "  * Image generation: The json may contain the relative path to the generated images. Imagine the packaged model is Super Resolution model that generates five super resolution images. The package may store a folder named `generated_sr_image` in the output temporary file with this five images. Hence the **output.json** should be as following:\n",
    "\n",
    "```python\n",
    "{\n",
    " [\n",
    "   \"generated_sr_image/image_1.png\",\n",
    "   \"generated_sr_image/image_2.png\",\n",
    "   \"generated_sr_image/image_3.png\",\n",
    "   \"generated_sr_image/image_4.png\",\n",
    "   \"generated_sr_image/image_5.png\",\n",
    " ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11010bc-8b90-4a52-82c3-82648f8a7b28",
   "metadata": {},
   "source": [
    "## 1) Pre-Processing\n",
    "\n",
    "### Sanity check and statistics\n",
    "\n",
    "**SanityCheck** and **DSStatistics** are the classes that will perform sanity check and statistics of image datasets and ground truth. They are stand alone classes, it is to say they can work by proving the path folder of images and ground truth, or they can work with **DSWrapper** class.\n",
    "\n",
    "#### Sanity check\n",
    "\n",
    "The SanityCheck module performs sanity to image datasets and ground truth. It can either work as standalone class or with DSWrapper class. It will remove all corrupted samples following the logic in the argument flags. The new sanitized dataset is located in output_path attribute from the SanityCheck instance. \n",
    "A usage example:\n",
    "\n",
    "```python\n",
    "from iquaflow.sanity import SanityCheck\n",
    "\n",
    "sc = SanityCheck(data_path, output_folder)\n",
    "sc.check_annotations()\n",
    "```\n",
    "\n",
    "Some relevant taskes performed are:\n",
    "\n",
    " * Finding duplicates in coco json images list\n",
    " * Check if the image format is a valid image file format.\n",
    " * Check integrity of one coco annotation.\n",
    " * Fix height and width in coco json images list\n",
    " * In geojson annotations, remove all rows containing a Nan value, empty geometries in any of the required field columns.\n",
    " * In geojson annotations, try to fix geometries with buffer = 0 and remove the persistent invalid geometries.\n",
    "\n",
    "Note the difference between missing, empty and invalid geometries in a geojson:\n",
    "\n",
    " * _Missing geometries:_ This is when the attribute geometry is empty or unknown. Most libraries load it as `None` type in python. These values were typically propagated in operations (for example in calculations of the area or of the intersection), or ignored in reductions such as unary_union.\n",
    " * _Empty geometries:_ This happens when the coordinates are empty despite having a geometry type defined. This can happen as a result of an intersection between two polygons that have no overlap.\n",
    " * _Invalid geometry:_ Problematic features such as edges of a polygon intersecting themselves. This could have happened due to a mistake from the annotator. For the case of invalid geometry. The tool will also attempt to fix them with buffer=0 functionality prior to removing. In future releases an additional argument to simplify geometries will be offered.\n",
    "\n",
    "\n",
    "\n",
    "#### Statistics and exploration\n",
    "\n",
    "There are several statistics that can be calculated from the datasets, they can be estimated and summariezed in visualizations. The resulting calculated parameters can be exported as json and the plots as images. The default location is in a subfolder *stats* within the dataset. The module *DsStats* performs stats to image datasets and annotations. It can either work as standalone class or with DSWrapper class.\n",
    "A usage example:\n",
    "\n",
    "```python\n",
    "from iquaflow.ds_stats import DsStats\n",
    "\n",
    "dss = DsStats(data_path, output_folder)\n",
    "stats = dss.perform_stats(show_plots = True)\n",
    "```\n",
    "\n",
    "Statistics performed are:\n",
    "\n",
    " * Average height and width images\n",
    " * Class tags histogram\n",
    " * Image and bounding box aspect ratio and area histograms\n",
    " * Calculates the best fitting bounding box and rotated bounding box\n",
    " * High, width angle from bounding box and rotated bounding box\n",
    " * Compactness, centroid and area of the polygon\n",
    " * min, mean and max from a dataframe field\n",
    "\n",
    "There are also two interactive exploratory tools. One to visualzie the annotations an another for the images. These are:\n",
    "\n",
    " * notebook_annots_summary\n",
    " * notebook_imgs_preview\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "from iquaflow.ds_stats import DsStats\n",
    "\n",
    "DsStats.notebook_annots_summary(\n",
    "    df,\n",
    "    export_html_filename=html_filename,\n",
    "    fields_to_include=[\"image_filename\", \"class_id\", \"area\"],\n",
    "    show_inline=True,\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "from iquaflow.ds_stats import DsStats\n",
    "\n",
    "DsStats.notebook_imgs_preview(\n",
    "        data_path=data_path,\n",
    "        sample=100,\n",
    "        size=100,\n",
    ")\n",
    "```\n",
    "\n",
    "They can be used in line in notebooks or export them in html interactively.\n",
    "\n",
    "[See a notebook with Statistics examples](https://github.com/satellogic/iquaflow/tree/main/notebooks/Statistics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6618e-d491-487a-94c8-992080d75da8",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**DSWrapper** is the class that iquaflow uses for identifying datasets. Basically the dateset is defined by a folder that contains only a unique sub-folder with the images and json that describes the annotations. It is preferred that the ground truth json is in COCO format or geojson so it can be used with the rest of the tools.   \n",
    "\n",
    "Having the dataset conformed as mentioned before it is simply as providing the location path to the **DSWrapper**\n",
    "\n",
    "```python\n",
    "from iquaflow.datasets import DSWrapper\n",
    "ds_wrapper = DSWrapper(data_path=\"[path_to_the_dataset]\")\n",
    "```\n",
    "Internally iquaflow parses the structure helping the experiment tools to understand how the dateset is conformed.\n",
    "\n",
    "Afterwards the user can find parsed the principal datasets paths:\n",
    "\n",
    "```python\n",
    "ds_wrapper.parent_folder # It is Path of the folder containing the dataset\n",
    "ds_wrapper.data_path #Root path of the dataset\n",
    "ds_wrapper.data_input #Path of the folder that contains the images\n",
    "ds_wrapper.json_annotations #Path to the jsn annotations. Preferred COCO annotations\n",
    "ds_wrapper.geojson_annotations #Path to the geojson annotations.\n",
    "```\n",
    "Furthermore, **DSWrapper** contains an editable dictionary that describes the dataset. Initially this dictionary contains the key ds_name that is the name of the dataset. The user can populate this dictionary with any key/value parameter. Afterwards, this dictionary will be populated and changed automatically by **DSModifier** classes and it will be used for experiments logins.\n",
    "\n",
    "```python\n",
    "ds_wrapper.params #Contains metainfomation of the dataset. Initially {\"ds_name\":\"[name_of_the_dataset]\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b88aa-f593-41ae-92a2-4c091f7ec02f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modifiers\n",
    "\n",
    "Modifiers take a dataset D and process to obtain a D' dataset with some image/data processing (degradation, compression, enhancement...)\n",
    "\n",
    "#### Using an existing modifier to run an experiment:\n",
    "\n",
    "Just import the desired modifier and run it\n",
    "```python\n",
    "from iquaflow.datasets import DSModifier_jpg\n",
    "\n",
    "img_path = \"test_datasets/ds_coco_dataset/images\")\n",
    "jpg85 = DSModifier_jpg(params={\"quality\": 85})\n",
    "jpg85.modify(data_input=img_path)\n",
    "```\n",
    "\n",
    "After running, a `test_datasets/ds_coco_dataset#jpg85_modifier/images/` folder should be created with the modified images.\n",
    "\n",
    "#### Adding a new modifier tool:\n",
    "In [modifier_jpg.py](https://github.com/satellogic/iquaflow/tree/main/iquaflow/datasets/modifier_jpg.py) you have a good guide on how to implement a new modifier, inheriting from DSModifier_dir and writing the internal `_mod_img()` member function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f04a6-60dc-4863-93f4-990d22af3f5b",
   "metadata": {},
   "source": [
    "## 2) Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd565b82-a028-479b-b325-b7c0fad4b18c",
   "metadata": {},
   "source": [
    "### TaskExecution\n",
    "\n",
    "iquaflow can to automatize experiments while the user has a flexible way of loging experiments information\n",
    "without knowing any specific login tool, he needs only to create a json file with the parameters that want to be tracked by iquaflow. Alternatively he can track \n",
    "any kind of file generated by the experiment by just saving the file in a temporary path (provided to the packaged model by iquaflow) \n",
    "or he even can store the raw results in a json for future computations.\n",
    "**TaskExecution** is the generic class that provides the mandatory and optional arguments to the packaged model when this is launched and it \n",
    "is also responable for translating all the experiment information to the mlflow tracking server. Hence, the user does not need\n",
    "to understand **MLFlow**, iquaflow internally uses **MLFlow** to organize the experiments.\n",
    "\n",
    "#### PythonScriptTaskExecution\n",
    "\n",
    "This particular class extends from **TaskExecution** and knows how to execute a model that is encapsulated in a python script.\n",
    "In order to use it just instantiate the class with the path to the python script.\n",
    "\n",
    "```python\n",
    "task = PythonScriptTaskExecution(model_script_path=\"./path_to_script.py\")\n",
    "```\n",
    "\n",
    "Alternatively the user can execute the task, but is not recommendable since iquaflow will perform executions \n",
    "internally when the whole experiment is defined. In order to execute the run, the user must provide the experiment name,\n",
    "the name of the run  and the training dateset path or training **DSWrapper**. Optionally, the user can provide a training dataset path or ds_wrapper\n",
    "and a python dictionary with model hyper-parameters (that will be used when executing the package) \n",
    "\n",
    "```python\n",
    "task.train_val(\n",
    "            experiment_name=\"name of the experiment\",\n",
    "            run_name=\"test_run\",\n",
    "            train_ds=ds_wrapper_train,\n",
    "            val_ds=ds_wrapper_validation,\n",
    "            mlargs={\"lr\": 1e-6},\n",
    "        )\n",
    "```\n",
    "\n",
    "#### SageMakerTaskExecution\n",
    "\n",
    "Our application can run in sagemaker by passing a **SageMakerEstimatorFactory** as an argument of our **TaskExecution**. In which case it becomes a **SageMakerTaskExecution**. See an example on how to define it.\n",
    "\n",
    "```python\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from iquaflow.experiments.task_execution import SageMakerEstimatorFactory, SageMakerTaskExecution\n",
    "\n",
    "sage_estimator_factory = SageMakerEstimatorFactory(\n",
    "   PyTorch,\n",
    "   {\n",
    "       \"entry_point\": \"train.py\",\n",
    "       \"source_dir\": \"yolov5\",\n",
    "       \"role\":role,\n",
    "       \"framework_version\": \"1.8.1\",\n",
    "       \"py_version\": \"py3\",\n",
    "       \"instance_count\": 1,\n",
    "       \"instance_type\": \"ml.g4dn.xlarge\"\n",
    "   }\n",
    ")\n",
    "\n",
    "task = SageMakerTaskExecution( sage_estimator_factory )\n",
    "```\n",
    "\n",
    "Then in your training script, you might want to connect the argument script variables that are defined by convention in iquaflow (see Conventions) to SageMaker environmental variables to take full advantage of the SageMaker tools. As an example:\n",
    "\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Define some defaults\n",
    "trainds_default     = (os.environ[\"SM_CHANNEL_TRAINDS\"] if \"SM_CHANNEL_TRAINDS\" in os.environ else \"\")\n",
    "valds_default      = (os.environ[\"SM_CHANNEL_VALDS\"] if \"SM_CHANNEL_VALDS\" in os.environ else \"\")\n",
    "outputpath_default = (os.environ[\"SM_OUTPUT_DATA_DIR\"] if \"SM_OUTPUT_DATA_DIR\" in os.environ else \"./output\")\n",
    "\n",
    "# IQF arguments\n",
    "parser.add_argument(\"--trainds\", default=trainds_default, type=str, help=\"training dataset path\")\n",
    "parser.add_argument(\"--valds\", default=valds_default, type=str, help=\"validation dataset path\")\n",
    "parser.add_argument(\"--outputpath\", default=outputpath_default, type=str, help=\"path output\")\n",
    "```\n",
    "\n",
    "Also, for these approaches you might want iquaflow to upload the modifed datasets (by iquaflow-modifiers) on a bucket on the fly. To do so, indicate the **bucket_name** in the **cloud_options** whithin **ExperimentSetup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19612148-9ce4-4ed0-964e-d7bb61fa3123",
   "metadata": {},
   "source": [
    "### ExperimentSetup\n",
    "\n",
    "iquaflow allows to formulate experiments taking as reference the modified training datase . In order to perform this task,\n",
    "the package provides tools that allows to automatize this kind of experiments that is composed by: \n",
    "\n",
    "* A reference dataset.\n",
    "* A list of dataset modifiers.\n",
    "* A encapsuled machine learning model.\n",
    "\n",
    "The first two components are covered by **DSWrapper** and **DSModifer** respectively.\n",
    "The last one requires a **Task Execution**\n",
    "\n",
    "Having defined all the components the user is able to perform a iquaflow experiment by using **ExperimentSetup**.\n",
    "The user must define the name of the experiment, the reference datasets, the list of datasets modifiers \n",
    "and the packaged model, as following\n",
    "\n",
    "```python\n",
    "experiment = ExperimentSetup(\n",
    "   experiment_name=\"experimentA\",\n",
    "   task_instance=PythonScriptTaskExecution(model_script_path=\"./path_to_script.py\"),\n",
    "   ref_dsw_train=DSWrapper(data_path=\"path_to_dataset\"),\n",
    "   ds_modifiers_list=[ DSModifier_jpg(params={'quality': i}) for i in [10,30,50,70,90] ]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "And then just execute the training by \n",
    "\n",
    "```python\n",
    "experiment.execute()\n",
    "```\n",
    "\n",
    "#### additional options\n",
    "\n",
    "* **repetitions**\n",
    "\n",
    "Each combination of parameters and modifiers results in a run. Scipts might contain randomness (i.e. Random partitions). For those cases you might want to average out several executions to have a relevant statistic or study the variability. To do so, set the number of repetitions to greater than 1.\n",
    "\n",
    "* **mlflow_monitoring**\n",
    "\n",
    "This allows monitoring in real time of the training scripts. When turned on, iquaflow will pass these aditional arguments to the training script:\n",
    "\n",
    "     - mlfuri\n",
    "     - mlfexpid\n",
    "     - mlfrunid\n",
    " \n",
    "Thus, the user will be responsible to add these in the user training script when required.\n",
    "Then the user can activate the current experiment and run in the the script with a snippet such as:\n",
    "\n",
    "```python\n",
    "mlflow.set_tracking_uri(args.mlfuri)\n",
    "\n",
    "mlflow.start_run(\n",
    "    run_id=args.mlfrunid,\n",
    "    experiment_id=args.mlfexpid\n",
    ")\n",
    "```\n",
    "\n",
    "* cloud options\n",
    "\n",
    "**cloud_options** is a dictionary of options useful for indicating endpoints such as:\n",
    " \n",
    "     - bucket_name – str. If set, modified data (by iquaflow-modifiers) will be uploaded to the bucket. \n",
    "     - tracking_uri – str. trackingURI for mlflow. default is local to the ./mlflow folder\n",
    "     - registry_uri – str. registryURI for mlflow. default is local to the ./mlflow folder\n",
    "     \n",
    "Inicating the bucket is useful for **SageMakerTaskExecution** instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02536e-4974-4fda-93f6-09baf42f331c",
   "metadata": {},
   "source": [
    "## 3) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb43e4-22e1-4f0d-95c8-c262a724eb1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment Info\n",
    "\n",
    "This objects allows the user to manage the experiment information. It simplifies the access to MLFlow and allows to apply new metrics to previous executed experiments. Basic usage example:\n",
    "\n",
    "```python\n",
    "from iquaflow.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "runs = experiment_info.get_mlflow_run_info() # runs is a python dict\n",
    "```\n",
    "\n",
    "These are the main methods:\n",
    "\n",
    " * get_mlflow_run_info > It gathers the experiment information ina a python dictionary.\n",
    " * apply_metric_per_run > Applies a new metric to previously executed experiments.\n",
    " * get_df > Retrives a selection of data in a suitable format so that it can be used as an input in the Visualization module.\n",
    "\n",
    "In the section Metrics and Visualization (just below) there are examples on how to use the last two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb6690-1887-42fd-a621-1c3d7b8d03f9",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The module metrics contains functionalities to estimate metrics in your experiments. \n",
    "*BBDetectionMetrics* is an available metric that can be applied between bounding boxes of ground truth and predicted elements. They must be in COCO-format ( See [COCO detection](https://cocodataset.org/#detection-eval) and [COCO data](https://cocodataset.org/#format-data) ). When this metric is applied the metrics from COCOeval (See [COCO detection](https://cocodataset.org/#detection-eval) ) are estimated.\n",
    "\n",
    "#### SNRMetric\n",
    "\n",
    "Signal-to-noise ratio is defined as the ratio of the power of a signal to the power of background noise.\n",
    "This metric is designed for L0 - L1 images. There are currently two approaches to estimate it:\n",
    "* Homogeneous blocks (HB) - default option, faster and less problematic.\n",
    "* Homogeneous areas (HA) - usually more accurate. \n",
    "\n",
    "```python\n",
    "from iquaflow.metrics import (\n",
    "       SNRMetric,\n",
    "       snr_function_from_array,\n",
    "       snr_function_from_fn\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "#### SharpnessMetric\n",
    "\n",
    "About RER, FWHM and MTF. In general the MTF can be ignored because it is the most complex and the least reliable. With just a bit of noise in the data the metric changes a lot and in this case the images are noisy.\n",
    "\n",
    "```python\n",
    "from iquaflow.metrics import SharpnessMetric\n",
    "```\n",
    "\n",
    "RER - It measures the slope in the edge response (transition). The lower the metric, the blurier the image is. Taking the derivative of normalized Edge Response produces the Line Spread Function (LSF). The LSF is a 1-D representation of the system PSF. The width of the LSF at half the height (the 50% point) is called the full-width at half maximum (FWHM).\n",
    "\n",
    "The FWHM (Full Width at Half Maximum)  measures the level of blur. It has three measurements depending on the direction.\n",
    "* FWHM_X - Horizontal direction of the image\n",
    "* FWHM_Y - Vertical direction of the image\n",
    "* FWHM_other - The rest. Grouped together. This one is less reliable because it depends on the content of the image such as how many angles there are.\n",
    "\n",
    "The Fourier Transform of the LSF produces the Modulation Transfer Function (MTF). MTF is determined across all spatial frequencies, but can be evaluated at a single spatial frequency, such as the Nyquist frequency. The value of the MTF at Nyquist provides a measure of resolvable contrast at the highest ‘alias-free’ spatial frequency.\n",
    "\n",
    "\n",
    "#### BBDetectionMetrics\n",
    "\n",
    "```python\n",
    "from iquaflow.metrics import BBDetectionMetrics\n",
    "```\n",
    "\n",
    "This estimates object detection metrics (Recall, mAP, etc.) over a dataset that has its predictions in COCO-inference format (See conventions)\n",
    "\n",
    "\n",
    "#### Custom metrics\n",
    "\n",
    "\n",
    " - Custom metrics can be created by inheriting the class *Metrics*:\n",
    "\n",
    "```python\n",
    "from iquaflow.metrics import Metric\n",
    "\n",
    "class CustomMetric(Metric):\n",
    "    def __init__(self) -> None:\n",
    "        self.metric_names = coco_eval_metrics_names\n",
    "    def apply(self, predictions: str, gt_path: str) -> Any:\n",
    "        # Your custom code here\n",
    "        # Then return a dictionary of names and values for each metric\n",
    "        return {k: v for k, v in zip(metric_names, stats)}\n",
    "```\n",
    "\n",
    " - To calculate a metric to an executed experiment do:\n",
    "\n",
    "```python\n",
    "from iquaflow.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "my_custom_metric = CustomMetric()\n",
    "experiment_info.apply_metric_per_run( my_custom_metric, json_annotations_name )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85b9d8-887c-4b7e-8ca2-78fec0831f85",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Apart from the visualization tools explained in the Sanity check and Statistics section, there are also tools for plotting the results. On one hand there is the [mlflow](https://mlflow.org/) service which is launched by `mlflow ui --host 0.0.0.0` and then accessed in the browser `http://ip_address_of_your_mlflow_server:5000` The Tracking UI lets you visualize, search and compare runs, as well as download run artifacts or metadata for analysis in other tools. If you log runs to a local mlruns directory, run mlflow ui in the directory above it, and it loads the corresponding runs. The UI contains the following key features:\n",
    " * Experiment-based run listing and comparison\n",
    " * Searching for runs by parameter or metric value\n",
    " * Visualizing run metrics\n",
    " * Downloading run results\n",
    "\n",
    "On the other hand there is the *ExperimentVisual* class. It offers both inline and saved files plotting utilities. It is designed so that it retrieves a dataframe extracted from an *ExperimentInfo* and then used as an input. See some examples in [Visual Notebooks](https://github.com/satellogic/iquaflow/tree/main/notebooks/Visualizations.ipynb). See also some code examples in the Typical workflow section (just below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec56524-aa32-4094-8735-829ea0f0cf02",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "### Package Overview\n",
    "\n",
    "The python package structure of this tool box is based on [cookiecutter](http://gitlab.local/juanlu/cookiecutter-satellogic-ground). This library provides a standard workflow for developing production level packages. The tools that\n",
    "will be used are:\n",
    "1. [setuptools](https://pypi.org/project/setuptools/) for packaging\n",
    "1. [versioneer](https://pypi.org/project/versioneer/) for versioning\n",
    "1. [GitLab CI](https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/) for continuous integration\n",
    "1. [tox](https://pypi.org/project/tox/3.2.0/) for managing test environments\n",
    "1. [pytest](https://pypi.org/project/pytest/) for tests\n",
    "1. [sphinx](https://pypi.org/project/Sphinx/) for documentation\n",
    "1. [black](https://pypi.org/project/black/), [flake8](https://pypi.org/project/flake8/2.0/) and [isort](https://pypi.org/project/isort/) for style checks\n",
    "1. [mypy](https://pypi.org/project/mypy/) for type checks\n",
    "\n",
    "More information can be found in:\n",
    "1. https://packaging.python.org/tutorials/packaging-projects/\n",
    "1. https://python-packaging.readthedocs.io/en/latest/minimal.html\n",
    "1. https://www.learnpython.org/en/Modules_and_Packages\n",
    "\n",
    "### Environment installation\n",
    "\n",
    "This repository does not require any specific python environment. In our case we use Python 3.7. Hence, we do recommend create a new environment with Python 3.7 and pip. The file ​*setup.py​* allows to install ​*iquaflow​* as a python package via pip. Once you have created your new environment, you only need to clone locally the repository:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/satellogic/iquaflow\n",
    "\n",
    "```\n",
    "and then do the wallowing command to install the *iquaflow* as a softlink in\n",
    "the environment:\n",
    "```bash\n",
    "python -m pip install -e .\n",
    "```\n",
    "Dependencies are defined in ​*setup.cfg* ​under ​install_requires​ tag. So first install the package\n",
    "in your local environment and then add the dependency in the ​*setup.cfg* ​with its corresponding\n",
    "version.\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "We use Sphinx to automatically update our documentation. This allows to maintain package documentation\n",
    "updated at the same time new code is added (as long the code is commented). The documentation and Sphinx configuration can be found inside ​/doc​. \n",
    "\n",
    "Under the ​/doc ​folder type in console\n",
    "```bash\n",
    "make html\n",
    "```\n",
    "Sphinx will generate under ​/doc/build/html ​the desired html documentation.\n",
    "You can also use tox:\n",
    "```bash\n",
    "tox -e docs\n",
    "```\n",
    "\n",
    "More information ​about Sphinx can be found in [here](https://www.writethedocs.org/guide/tools/sphinx/).\n",
    "\n",
    "### Continuous integration\n",
    "\n",
    "In our project we use TOX. This tool allows to manage multiple environments in order to automatically validate code. More information about TOX can be found in [here](https://github.com/tox-dev/tox).\n",
    "\n",
    "For quality check you only need to run:\n",
    "```bash\n",
    "tox -e check\n",
    "```\n",
    "For automatic code reformat:\n",
    "```bash\n",
    "tox -e reformat\n",
    "```\n",
    "For executing all test for first time use\n",
    "```bash\n",
    "tox -r -e py36\n",
    "```\n",
    "Alternatively, if it is not the first time it is not necesary to recreate the tox envirement\n",
    "\n",
    "```bash\n",
    "tox -e py36\n",
    "```\n",
    "\n",
    "Note: CI terminology for python can be found in [here](https://www.patricksoftwareblog.com/setting-up-gitlab-ci-for-a-python-application/)\n",
    "\n",
    "### Test\n",
    "\n",
    "Unit tests are performed using [PyTest](https://docs.pytest.org/en/stable/usage.html​). All tests are included in ​test the folder located in the repository main folder.\n",
    "Once you have created a new test module, *e.g.* test_new_module, that includes python assertions, simply type in the console *pytest* or: \n",
    "```bash\n",
    "pytest <module name>\n",
    "```\n",
    "to run the tests. \n",
    "\n",
    "We strongly recommned to use “test_” as the prefix of every test you create. \n",
    "\n",
    "You can also run test manually using *tox*(recommended) (use *-r* parametar for creating tox environment for the first time): ​\n",
    "```bash\n",
    "tox -e py36\n",
    "```\n",
    "More information ​can be found in https://docs.python-guide.org/writing/tests/\n",
    "\n",
    "\n",
    "### Initial development process\n",
    "\n",
    "Below we describe usual steps when developing from scratch:\n",
    "\n",
    "1. Setup python environment:\n",
    "  \n",
    "   ```bash\n",
    "   conda create -n iqt-env python=3.6\n",
    "   ```\n",
    "\n",
    "1. Clone repository:\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/satellogic/iquaflow\n",
    "   ```\n",
    "\n",
    "1. Create branch:\n",
    "\n",
    "   ```bash\n",
    "   git checkout -b <new_branch_name>\n",
    "   ```\n",
    "\n",
    "1. Install soft link via:\n",
    "\n",
    "   ```bash\n",
    "   python -m pip install -e . \n",
    "   ```\n",
    "\n",
    "1. Create test that defines modules functionality.\n",
    "1. Solve the test by adding package functionality.\n",
    "1. If new branch pulled use `tox -r` to recreate tox environments.\n",
    "1. Reformat code:\n",
    "   ```bash\n",
    "   python -m pip install tox \n",
    "   tox -e reformat\n",
    "   ```\n",
    "1. Check code and solve:\n",
    "   \n",
    "   ```bash\n",
    "   tox -e check\n",
    "   ```\n",
    "\n",
    "1. Run tests:\n",
    "\n",
    "   ```bash\n",
    "   tox -e py36\n",
    "   ```\n",
    "\n",
    "1. Push to remote branch.\n",
    "1. Create MR and assign reviewer.\n",
    "1. Refreshing local repository for running tests (after `pip install -e .`):\n",
    "   \n",
    "   ```bash\n",
    "   tox -r -e py36Sphinx\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
